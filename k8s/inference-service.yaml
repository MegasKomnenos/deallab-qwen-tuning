apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: qwen-chat
  namespace: kubeflow-user-example-com
  annotations:
    serving.kserve.io/deploymentMode: RawDeployment 
spec:
  predictor:
    model:
      modelFormat:
        name: vllm
      runtime: kserve-vllm-custom
      storageUri: "pvc://llm-workspace-pvc/Qwen--Qwen3-4B-Thinking-2507"
      
      # Resources for this specific model instance
      resources:
        limits:
          cpu: "4"
          memory: 16Gi
          nvidia.com/gpu: "1"
        requests:
          cpu: "2"
          memory: 8Gi
          nvidia.com/gpu: "1"

      env:
        - name: VLLM_USE_V1
          value: "0"
        - name: VLLM_ATTENTION_BACKEND
          value: "XFORMERS"
        - name: HF_HUB_DISABLE_TELEMETRY
          value: "1"

      args:
        - --port=8080
        - --model=/mnt/models
        - --served-model-name=qwen
        - --gpu-memory-utilization=0.95
        - --max-model-len=6144
        - --trust-remote-code
        - --dtype=float16
---
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: qwen-chat-direct
  namespace: kubeflow-user-example-com
spec:
  gateways:
    - kubeflow/kubeflow-gateway
  
  hosts:
    - "qwen-chat-kubeflow-user-example-com.example.com"
  
  http:
    - match:
        - uri:
            prefix: /
      
      route:
        - destination:
            host: qwen-chat-predictor.kubeflow-user-example-com.svc.cluster.local
            port:
              number: 80
      
      timeout: 300s
