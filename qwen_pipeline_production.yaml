# PIPELINE DEFINITION
# Name: qwen-finetune-production
# Inputs:
#    data_pvc: str [Default: 'llm-data-pvc']
#    force_download: bool [Default: True]
#    max_steps: int [Default: 50.0]
#    model_name: str [Default: 'Qwen/Qwen3-4B-Thinking-2507']
#    model_pvc: str [Default: 'llm-workspace-pvc']
#    playback_pvc: str [Default: 'llm-playback-pvc']
#    training_image_uri: str [Default: 'kjh123456/qwen-trainer:v27']
components:
  comp-download-dataset:
    executorLabel: exec-download-dataset
    inputDefinitions:
      parameters:
        data_root:
          parameterType: STRING
        dataset_name:
          parameterType: STRING
        force_download:
          parameterType: BOOLEAN
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-download-model:
    executorLabel: exec-download-model
    inputDefinitions:
      parameters:
        force_download:
          parameterType: BOOLEAN
        model_name:
          parameterType: STRING
        model_root:
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-download-playback:
    executorLabel: exec-download-playback
    inputDefinitions:
      parameters:
        force_download:
          parameterType: BOOLEAN
        playback_root:
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-launch-training-job:
    executorLabel: exec-launch-training-job
    inputDefinitions:
      parameters:
        base_model_path:
          parameterType: STRING
        data_path:
          parameterType: STRING
        data_pvc:
          parameterType: STRING
        image:
          parameterType: STRING
        max_steps:
          defaultValue: 1.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        model_pvc:
          parameterType: STRING
        model_root:
          parameterType: STRING
        playback_path:
          parameterType: STRING
        playback_pvc:
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-merge-adapter:
    executorLabel: exec-merge-adapter
    inputDefinitions:
      parameters:
        adapter_path:
          parameterType: STRING
        base_model_path:
          parameterType: STRING
        merged_root:
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-run-inference:
    executorLabel: exec-run-inference
    inputDefinitions:
      parameters:
        model_path:
          parameterType: STRING
        prompt:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-download-dataset:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - download_dataset
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'datasets==2.19.0'\
          \ 'huggingface_hub' 'scipy' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef download_dataset(\n    data_root: str,\n    dataset_name: str,\n\
          \    force_download: bool\n) -> str:\n    import os\n    import shutil\n\
          \    from datasets import load_dataset, Dataset\n    import time\n    start_time\
          \ = time.time()\n\n    save_path = os.path.join(data_root, \"raw\", \"pg19\"\
          )\n\n    # ... (Cache check logic remains the same) ...\n    if os.path.exists(save_path):\n\
          \        if force_download:\n            shutil.rmtree(save_path)\n    \
          \    else:\n            print(f\"Large cached dataset found at {save_path}.\
          \ Skipping download.\")\n            return save_path\n\n    ds = load_dataset(\n\
          \        dataset_name,\n        split=\"train\",\n        streaming=True\n\
          \    )\n\n    CHARS_PER_SAMPLE = 4096\n\n    def process_batch(items):\n\
          \        out = []\n        for text in items['text']:\n            if len(text)\
          \ < CHARS_PER_SAMPLE:\n                continue\n\n            buf = []\n\
          \n            i, j = (0, CHARS_PER_SAMPLE)\n\n            while True:\n\
          \                cur = text[i:j]\n\n                if not cur:\n      \
          \              break\n\n                k = max(1, len(cur)//2)\n      \
          \          buf.append([\n                    {\"role\": \"user\", \"content\"\
          : cur[:k]},\n                    {\"role\": \"assistant\", \"content\":\
          \ cur[k:]} \n                ])\n                i = i+k\n             \
          \   j = min(len(text), j+k)\n                if 1.75*(j - i) < CHARS_PER_SAMPLE:\n\
          \                    buf[-1][-1][\"content\"] += text[i:j]\n           \
          \         break\n\n            out.extend(buf)\n\n        return { \"messages\"\
          : out, \"type\": [\"style\"] * len(out) }\n\n    final_ds = ds.select_columns([\"\
          text\"])\n    final_ds = final_ds.map(process_batch, batched=True, batch_size=16,\
          \ remove_columns=[\"text\"])\n    final_ds = final_ds.select_columns([\"\
          messages\", \"type\"])\n    os.makedirs(save_path, exist_ok=True)\n    final_ds.save_to_disk(save_path)\n\
          \n    print(f\"\\n--- Finished in {(time.time() - start_time)/60:.2f} minutes\
          \ ---\")\n\n    return save_path\n\n"
        image: python:3.10
    exec-download-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - download_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'huggingface_hub'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef download_model(model_name: str, model_root: str, force_download:\
          \ bool) -> str:\n    import os\n    import shutil\n    from huggingface_hub\
          \ import snapshot_download\n    import time\n    start_time = time.time()\n\
          \    safe_name = model_name.replace(\"/\", \"--\")\n    save_path = os.path.join(model_root,\
          \ \"base_models\", safe_name)\n    if os.path.exists(save_path) and force_download:\n\
          \        shutil.rmtree(save_path)\n    snapshot_download(repo_id=model_name,\
          \ local_dir=save_path, local_dir_use_symlinks=False)\n\n    print(f\"\\\
          n--- Finished in {(time.time() - start_time)/60:.2f} minutes ---\")\n\n\
          \    return save_path\n\n"
        image: python:3.10
    exec-download-playback:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - download_playback
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'datasets==2.19.0'\
          \ 'huggingface_hub' 'scipy' 'requests' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef download_playback(\n    playback_root: str,\n    force_download:\
          \ bool,\n) -> str:\n    import os\n    import shutil\n    from datasets\
          \ import load_dataset, Dataset\n    import time\n    import json\n    import\
          \ gzip\n    import requests\n\n    start_time = time.time()\n\n    save_path\
          \ = os.path.join(playback_root, \"raw\", \"oasst2\")\n\n    # ... (Cache\
          \ check logic remains the same) ...\n    if os.path.exists(save_path):\n\
          \        if force_download:\n            shutil.rmtree(save_path)\n    \
          \    else:\n            print(f\"Large cached dataset found at {save_path}.\
          \ Skipping download.\")\n            return save_path\n\n    url = \"https://huggingface.co/datasets/OpenAssistant/oasst2/resolve/main/2023-11-05_oasst2_ready.trees.jsonl.gz\"\
          \n\n    print(f\"Downloading and loading trees from {url}...\")\n\n    #\
          \ 1. Stream the file directly from the web\n    # 2. Decompress on the fly\
          \ using gzip\n    # 3. Parse each line as a JSON object\n    with requests.get(url,\
          \ stream=True) as r:\n        r.raise_for_status()\n        # 'rt' mode\
          \ opens the file in read-text mode, handling the gzip decompression\n  \
          \      with gzip.open(r.raw, \"rt\", encoding=\"utf-8\") as f:\n       \
          \     # Load all trees into a list\n            trees = [json.loads(line)\
          \ for line in f]\n\n    print(f\"Successfully loaded {len(trees)} trees.\"\
          )\n\n    role_map = {\n        \"prompter\": \"user\",\n        \"assistant\"\
          : \"assistant\"\n    }\n\n    def extract_conversation_paths(node, current_history=None):\n\
          \        \"\"\"\n        Recursive function to traverse the tree and extract\
          \ all linear \n        conversation paths (Root -> Leaf).\n        \"\"\"\
          \n        if current_history is None:\n            current_history = []\n\
          \n        if node['role'] == \"assistant\" and (not \"labels\" in node or\
          \ not \"quality\" in node[\"labels\"] or float(node[\"labels\"][\"quality\"\
          ][\"value\"]) < 0.5):\n            return []\n\n        # create the message\
          \ object for the current node\n        message = {\n            \"role\"\
          : role_map.get(node['role'], node['role']),\n            \"content\": node['text']\n\
          \        }\n\n        # Create a new history including this node\n     \
          \   new_history = current_history + [message]\n\n        # If this is a\
          \ leaf node (no replies), return the path\n        if not node.get('replies'):\n\
          \            return [new_history]\n\n        # If there are replies, recurse\
          \ down each branch\n        paths = []\n        for reply in node['replies']:\n\
          \            paths.extend(extract_conversation_paths(reply, new_history))\n\
          \        if len(paths) == 0:\n            paths = [new_history]\n      \
          \  return paths\n\n    all_conversations = []\n\n    print(\"Processing\
          \ trees into linear conversations...\")\n\n    for tree in trees:\n    \
          \    if tree[\"prompt\"][\"lang\"] != \"en\":\n            continue\n\n\
          \        paths = extract_conversation_paths(tree[\"prompt\"])\n\n      \
          \  # Optional: Filter for quality here (e.g., only keep paths ending in\
          \ assistant)\n        for path in paths:\n            while len(path) >\
          \ 0 and path[-1][\"role\"] != \"assistant\":\n                path.pop()\n\
          \n            if len(path) == 0:\n                continue\n\n         \
          \   all_conversations.append(path)\n\n    print(f\"Extracted {len(all_conversations)}\
          \ unique conversation paths.\")\n\n    # Create the Hugging Face Dataset\n\
          \    # We wrap the list in a dict structure: {'messages': [ ... ]}\n   \
          \ formatted_data = [{\"messages\": conv, \"type\": \"playback\"} for conv\
          \ in all_conversations]\n\n    hf_dataset = Dataset.from_list(formatted_data)\n\
          \    os.makedirs(save_path, exist_ok=True)\n    hf_dataset.save_to_disk(save_path)\n\
          \n    print(f\"\\n--- Finished in {(time.time() - start_time)/60:.2f} minutes\
          \ ---\")\n\n    return save_path\n\n"
        image: python:3.10
    exec-launch-training-job:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - launch_training_job
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'kubernetes'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef launch_training_job(\n    base_model_path: str,\n    data_path:\
          \ str,\n    playback_path: str,\n    model_root: str,\n    image: str,\n\
          \    model_pvc: str,\n    data_pvc: str,\n    playback_pvc: str,\n    max_steps:\
          \ int = 1\n) -> str:\n    import time\n    from kubernetes import client,\
          \ config\n    import time\n    start_time = time.time()\n\n    config.load_incluster_config()\n\
          \    api = client.CustomObjectsApi()\n\n    with open(\"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\
          ) as f:\n        namespace = f.read().strip()\n\n    job_name = f\"qwen-finetune-{int(time.time())}\"\
          \n    # Output dir must be inside the PVC mount\n    output_dir_internal\
          \ = f\"/mnt/models/checkpoints/{job_name}\"\n    output_dir_external = f\"\
          {model_root}/checkpoints/{job_name}\"\n\n    # Arguments passed strictly\
          \ to argparse in train.py\n    cmd_args = [\n        \"--base_model_path\"\
          , base_model_path,\n        \"--data_path\", data_path,\n        \"--playback_path\"\
          , playback_path,\n        \"--output_dir\", output_dir_internal,\n     \
          \   \"--max_steps\", str(max_steps)\n    ]\n\n    manifest = {\n       \
          \ \"apiVersion\": \"kubeflow.org/v1\",\n        \"kind\": \"PyTorchJob\"\
          ,\n        \"metadata\": {\"name\": job_name, \"namespace\": namespace},\n\
          \        \"spec\": {\n            \"pytorchReplicaSpecs\": {\n         \
          \       \"Master\": {\n                    \"replicas\": 1,\n          \
          \          \"template\": {\n                        \"metadata\": {\n  \
          \                          \"annotations\": {\n                        \
          \        \"sidecar.istio.io/inject\": \"false\"\n                      \
          \      }\n                        },\n                        \"spec\":\
          \ {\n                            \"containers\": [{\n                  \
          \              \"name\": \"pytorch\",\n                                \"\
          image\": image,\n                                \"args\": cmd_args,\n \
          \                               \"resources\": {\n                     \
          \               \"limits\": {\"nvidia.com/gpu\": 1, \"memory\": \"24Gi\"\
          , \"cpu\": \"4\"}, \n                                    \"requests\": {\"\
          cpu\": \"2\"}\n                                },\n                    \
          \            \"volumeMounts\": [\n                                    {\"\
          name\": \"models\", \"mountPath\": \"/mnt/models\"},\n                 \
          \                   {\"name\": \"data\", \"mountPath\": \"/mnt/data\"},\n\
          \                                    {\"name\": \"dshm\", \"mountPath\"\
          : \"/dev/shm\"}\n                                ]\n                   \
          \         }],\n                            \"volumes\": [\n            \
          \                    {\"name\": \"models\", \"persistentVolumeClaim\": {\"\
          claimName\": model_pvc}},\n                                {\"name\": \"\
          data\", \"persistentVolumeClaim\": {\"claimName\": data_pvc}},\n       \
          \                         {\"name\": \"playback\", \"persistentVolumeClaim\"\
          : {\"claimName\": playback_pvc}},\n                                {\"name\"\
          : \"dshm\", \"emptyDir\": {\"medium\": \"Memory\"}}\n                  \
          \          ]\n                        }\n                    }\n       \
          \         }\n            }\n        }\n    }\n\n    print(f\"Submitting\
          \ PyTorchJob {job_name}...\")\n    api.create_namespaced_custom_object(\"\
          kubeflow.org\", \"v1\", namespace, \"pytorchjobs\", manifest)\n\n    while\
          \ True:\n        time.sleep(30)\n        status = api.get_namespaced_custom_object_status(\"\
          kubeflow.org\", \"v1\", namespace, \"pytorchjobs\", job_name)\n        conds\
          \ = status.get(\"status\", {}).get(\"conditions\", [])\n        if conds:\n\
          \            last = conds[-1]\n            if last[\"type\"] == \"Succeeded\"\
          :\n                break\n            if last[\"type\"] == \"Failed\":\n\
          \                raise RuntimeError(f\"Job Failed: {last.get('message')}\"\
          )\n\n    print(f\"\\n--- Finished in {(time.time() - start_time)/60:.2f}\
          \ minutes ---\")\n\n    return output_dir_external\n\n"
        image: python:3.10
    exec-merge-adapter:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - merge_adapter
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'transformers'\
          \ 'torch' 'peft' 'accelerate' 'bitsandbytes' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef merge_adapter(base_model_path: str, adapter_path: str, merged_root:\
          \ str) -> str:\n    import torch\n    import os\n    import gc\n    from\
          \ transformers import AutoModelForCausalLM, AutoTokenizer\n    from peft\
          \ import PeftModel\n    import time\n    start_time = time.time()\n\n  \
          \  print(f\"--- Merging Adapter ---\")\n    print(f\"Base: {base_model_path}\"\
          )\n    print(f\"Adapter: {adapter_path}\")\n\n    # 1. Load Base Model in\
          \ FP16 on CPU\n    # Why CPU? Loading a 4B/7B model in FP16 takes 8GB/15GB\
          \ RAM. \n    # If we do this on an 11GB GPU, we might OOM before merging.\n\
          \    # System RAM is usually cheaper and larger.\n    print(\"Loading base\
          \ model to System RAM (CPU)...\")\n    base_model = AutoModelForCausalLM.from_pretrained(\n\
          \        base_model_path,\n        torch_dtype=torch.float16,\n        device_map=\"\
          cpu\", # Force CPU\n        low_cpu_mem_usage=True,\n        trust_remote_code=True\n\
          \    )\n\n    # 2. Load Adapter\n    print(\"Loading LoRA adapter...\")\n\
          \    model = PeftModel.from_pretrained(base_model, adapter_path)\n\n   \
          \ # 3. Merge\n    print(\"Merging weights...\")\n    model = model.merge_and_unload()\n\
          \n    # 4. Save\n    save_path = os.path.join(merged_root, \"qwen_merged_final\"\
          )\n    print(f\"Saving merged model to {save_path}...\")\n    model.save_pretrained(save_path)\n\
          \n    # Save tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(base_model_path,\
          \ trust_remote_code=True)\n    tokenizer.save_pretrained(save_path)\n\n\
          \    # Cleanup\n    del model\n    del base_model\n    gc.collect()\n\n\
          \    print(f\"\\n--- Finished in {(time.time() - start_time)/60:.2f} minutes\
          \ ---\")\n\n    return save_path\n\n"
        image: pytorch/pytorch:2.3.0-cuda12.1-cudnn8-runtime
        resources:
          memoryLimit: 25.769803776
          resourceMemoryLimit: 24Gi
    exec-run-inference:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - run_inference
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'transformers'\
          \ 'torch' 'accelerate' 'bitsandbytes' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef run_inference(model_path: str, prompt: str):\n    import torch\n\
          \    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\
          \    import time\n    start_time = time.time()\n    print(f\"Loading merged\
          \ model from {model_path}...\")\n\n    # Load in 4-bit for inference to\
          \ ensure it fits comfortably in 11GB\n    # even with long context windows.\n\
          \    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n    \
          \    bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.float16\n\
          \    )\n\n    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\
          \    model = AutoModelForCausalLM.from_pretrained(\n        model_path,\n\
          \        quantization_config=bnb_config,\n        device_map=\"auto\",\n\
          \        trust_remote_code=True\n    )\n\n    messages = [\n        {\"\
          role\": \"system\", \"content\": \"You are a scholar from the Victorian\
          \ era. Write in an archaic, formal tone.\"},\n        {\"role\": \"user\"\
          , \"content\": prompt}\n    ]\n\n    # Qwen chat template application\n\
          \    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\
          \n    print(f\"Input Prompt: {text}\")\n    inputs = tokenizer(text, return_tensors=\"\
          pt\").to(model.device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n\
          \            **inputs, \n            max_new_tokens=200,\n            do_sample=True,\n\
          \            temperature=0.7,\n            top_p=0.9\n        )\n\n    response\
          \ = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(\"\
          -\" * 30)\n    print(\"MODEL OUTPUT:\")\n    print(\"-\" * 30)\n    print(response)\n\
          \n    print(f\"\\n--- Finished in {(time.time() - start_time)/60:.2f} minutes\
          \ ---\")\n\n"
        image: pytorch/pytorch:2.3.0-cuda12.1-cudnn8-runtime
        resources:
          accelerator:
            count: '1'
            resourceCount: '1'
pipelineInfo:
  name: qwen-finetune-production
root:
  dag:
    tasks:
      download-dataset:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-download-dataset
        inputs:
          parameters:
            data_root:
              runtimeValue:
                constant: /mnt/data
            dataset_name:
              runtimeValue:
                constant: deepmind/pg19
            force_download:
              componentInputParameter: force_download
        taskInfo:
          name: download-dataset
      download-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-download-model
        inputs:
          parameters:
            force_download:
              componentInputParameter: force_download
            model_name:
              componentInputParameter: model_name
            model_root:
              runtimeValue:
                constant: /mnt/models
        taskInfo:
          name: download-model
      download-playback:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-download-playback
        inputs:
          parameters:
            force_download:
              componentInputParameter: force_download
            playback_root:
              runtimeValue:
                constant: /mnt/playback
        taskInfo:
          name: download-playback
      launch-training-job:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-launch-training-job
        dependentTasks:
        - download-dataset
        - download-model
        - download-playback
        inputs:
          parameters:
            base_model_path:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: download-model
            data_path:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: download-dataset
            data_pvc:
              componentInputParameter: data_pvc
            image:
              componentInputParameter: training_image_uri
            max_steps:
              componentInputParameter: max_steps
            model_pvc:
              componentInputParameter: model_pvc
            model_root:
              runtimeValue:
                constant: /mnt/models
            playback_path:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: download-playback
            playback_pvc:
              componentInputParameter: playback_pvc
        taskInfo:
          name: launch-training-job
      merge-adapter:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-merge-adapter
        dependentTasks:
        - download-model
        - launch-training-job
        inputs:
          parameters:
            adapter_path:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: launch-training-job
            base_model_path:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: download-model
            merged_root:
              runtimeValue:
                constant: /mnt/models
        taskInfo:
          name: merge-adapter
      run-inference:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-run-inference
        dependentTasks:
        - merge-adapter
        inputs:
          parameters:
            model_path:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: merge-adapter
            prompt:
              runtimeValue:
                constant: Explain the nature of electricity.
        taskInfo:
          name: run-inference
  inputDefinitions:
    parameters:
      data_pvc:
        defaultValue: llm-data-pvc
        isOptional: true
        parameterType: STRING
      force_download:
        defaultValue: true
        isOptional: true
        parameterType: BOOLEAN
      max_steps:
        defaultValue: 50.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      model_name:
        defaultValue: Qwen/Qwen3-4B-Thinking-2507
        isOptional: true
        parameterType: STRING
      model_pvc:
        defaultValue: llm-workspace-pvc
        isOptional: true
        parameterType: STRING
      playback_pvc:
        defaultValue: llm-playback-pvc
        isOptional: true
        parameterType: STRING
      training_image_uri:
        defaultValue: kjh123456/qwen-trainer:v27
        isOptional: true
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.13.0
---
platforms:
  kubernetes:
    deploymentSpec:
      executors:
        exec-download-dataset:
          pvcMount:
          - componentInputParameter: data_pvc
            mountPath: /mnt/data
        exec-download-model:
          pvcMount:
          - componentInputParameter: model_pvc
            mountPath: /mnt/models
        exec-download-playback:
          pvcMount:
          - componentInputParameter: playback_pvc
            mountPath: /mnt/playback
        exec-merge-adapter:
          pvcMount:
          - componentInputParameter: model_pvc
            mountPath: /mnt/models
        exec-run-inference:
          pvcMount:
          - componentInputParameter: model_pvc
            mountPath: /mnt/models
