# PIPELINE DEFINITION
# Name: qwen-finetune-production
# Inputs:
#    data_pvc: str [Default: 'llm-data-pvc']
#    dataset_name: str [Default: 'deepmind/pg19']
#    force_download: bool [Default: True]
#    max_steps: int [Default: 50.0]
#    model_name: str [Default: 'Qwen/Qwen3-4B-Thinking-2507']
#    model_pvc: str [Default: 'llm-workspace-pvc']
#    subset_size: int [Default: 500.0]
#    training_image_uri: str [Default: 'kjh123456/qwen-trainer:v16']
components:
  comp-download-dataset:
    executorLabel: exec-download-dataset
    inputDefinitions:
      parameters:
        data_root:
          parameterType: STRING
        dataset_name:
          parameterType: STRING
        force_download:
          parameterType: BOOLEAN
        subset_size:
          parameterType: NUMBER_INTEGER
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-download-model:
    executorLabel: exec-download-model
    inputDefinitions:
      parameters:
        force_download:
          parameterType: BOOLEAN
        model_name:
          parameterType: STRING
        model_root:
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-launch-training-job:
    executorLabel: exec-launch-training-job
    inputDefinitions:
      parameters:
        base_model_path:
          parameterType: STRING
        data_path:
          parameterType: STRING
        data_pvc:
          parameterType: STRING
        image:
          parameterType: STRING
        max_steps:
          defaultValue: 1.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        model_pvc:
          parameterType: STRING
        model_root:
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-merge-adapter:
    executorLabel: exec-merge-adapter
    inputDefinitions:
      parameters:
        adapter_path:
          parameterType: STRING
        base_model_path:
          parameterType: STRING
        merged_root:
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-run-inference:
    executorLabel: exec-run-inference
    inputDefinitions:
      parameters:
        model_path:
          parameterType: STRING
        prompt:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-download-dataset:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - download_dataset
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'datasets==2.19.0'\
          \ 'huggingface_hub' 'scipy' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef download_dataset(\n    dataset_name: str,\n    data_root: str,\n\
          \    force_download: bool,\n    subset_size: int\n) -> str:\n    import\
          \ os\n    import shutil\n    from datasets import load_dataset, Dataset\
          \ as HFDataset\n\n    save_path = os.path.join(data_root, \"raw\", \"pg19_large_cache\"\
          )\n\n    # ... (Cache check logic remains the same) ...\n    if os.path.exists(save_path):\n\
          \        if force_download:\n            shutil.rmtree(save_path)\n    \
          \    else:\n            print(f\"Large cached dataset found at {save_path}.\
          \ Skipping download.\")\n            return save_path\n\n    ds = load_dataset(\n\
          \        dataset_name,\n        split=\"train\",\n        streaming=True,\n\
          \        trust_remote_code=True\n    )\n\n    data_list = []\n    count\
          \ = 0\n\n    # Download a chunk\n    for item in ds:\n        if len(item['text'])\
          \ > 5000:\n            data_list.append({\"text\": item['text']})\n    \
          \        count += 1\n            if count >= subset_size:\n            \
          \    break\n\n    print(f\"Saving {count} books to disk...\")\n    final_ds\
          \ = HFDataset.from_list(data_list)\n    os.makedirs(save_path, exist_ok=True)\n\
          \    final_ds.save_to_disk(save_path)\n\n    return save_path\n\n"
        image: python:3.10
    exec-download-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - download_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'huggingface_hub'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef download_model(model_name: str, model_root: str, force_download:\
          \ bool) -> str:\n    import os\n    import shutil\n    from huggingface_hub\
          \ import snapshot_download\n    safe_name = model_name.replace(\"/\", \"\
          --\")\n    save_path = os.path.join(model_root, \"base_models\", safe_name)\n\
          \    if os.path.exists(save_path) and force_download:\n        shutil.rmtree(save_path)\n\
          \    snapshot_download(repo_id=model_name, local_dir=save_path, local_dir_use_symlinks=False)\n\
          \    return save_path\n\n"
        image: python:3.10
    exec-launch-training-job:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - launch_training_job
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'kubernetes'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef launch_training_job(\n    base_model_path: str,\n    data_path:\
          \ str,\n    model_root: str,\n    image: str,\n    model_pvc: str,\n   \
          \ data_pvc: str,\n    max_steps: int = 1\n) -> str:\n    import time\n \
          \   from kubernetes import client, config\n\n    config.load_incluster_config()\n\
          \    api = client.CustomObjectsApi()\n\n    with open(\"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\
          ) as f:\n        namespace = f.read().strip()\n\n    job_name = f\"qwen-finetune-{int(time.time())}\"\
          \n    # Output dir must be inside the PVC mount\n    output_dir_internal\
          \ = f\"/mnt/models/checkpoints/{job_name}\"\n    output_dir_external = f\"\
          {model_root}/checkpoints/{job_name}\"\n\n    # Arguments passed strictly\
          \ to argparse in train.py\n    cmd_args = [\n        \"--base_model_path\"\
          , base_model_path,\n        \"--data_path\", data_path,\n        \"--output_dir\"\
          , output_dir_internal,\n        \"--max_steps\", str(max_steps)\n    ]\n\
          \n    manifest = {\n        \"apiVersion\": \"kubeflow.org/v1\",\n     \
          \   \"kind\": \"PyTorchJob\",\n        \"metadata\": {\"name\": job_name,\
          \ \"namespace\": namespace},\n        \"spec\": {\n            \"pytorchReplicaSpecs\"\
          : {\n                \"Master\": {\n                    \"replicas\": 1,\n\
          \                    \"template\": {\n                        \"metadata\"\
          : {\n                            \"annotations\": {\n                  \
          \              \"sidecar.istio.io/inject\": \"false\"\n                \
          \            }\n                        },\n                        \"spec\"\
          : {\n                            \"containers\": [{\n                  \
          \              \"name\": \"pytorch\",\n                                \"\
          image\": image,\n                                \"args\": cmd_args,\n \
          \                               \"resources\": {\"limits\": {\"nvidia.com/gpu\"\
          : 1, \"memory\": \"16Gi\"}},\n                                \"volumeMounts\"\
          : [\n                                    {\"name\": \"models\", \"mountPath\"\
          : \"/mnt/models\"},\n                                    {\"name\": \"data\"\
          , \"mountPath\": \"/mnt/data\"},\n                                    {\"\
          name\": \"dshm\", \"mountPath\": \"/dev/shm\"}\n                       \
          \         ]\n                            }],\n                         \
          \   \"volumes\": [\n                                {\"name\": \"models\"\
          , \"persistentVolumeClaim\": {\"claimName\": model_pvc}},\n            \
          \                    {\"name\": \"data\", \"persistentVolumeClaim\": {\"\
          claimName\": data_pvc}},\n                                {\"name\": \"\
          dshm\", \"emptyDir\": {\"medium\": \"Memory\"}}\n                      \
          \      ]\n                        }\n                    }\n           \
          \     }\n            }\n        }\n    }\n\n    print(f\"Submitting PyTorchJob\
          \ {job_name}...\")\n    api.create_namespaced_custom_object(\"kubeflow.org\"\
          , \"v1\", namespace, \"pytorchjobs\", manifest)\n\n    while True:\n   \
          \     time.sleep(30)\n        status = api.get_namespaced_custom_object_status(\"\
          kubeflow.org\", \"v1\", namespace, \"pytorchjobs\", job_name)\n        conds\
          \ = status.get(\"status\", {}).get(\"conditions\", [])\n        if conds:\n\
          \            last = conds[-1]\n            if last[\"type\"] == \"Succeeded\"\
          :\n                break\n            if last[\"type\"] == \"Failed\":\n\
          \                raise RuntimeError(f\"Job Failed: {last.get('message')}\"\
          )\n\n    return output_dir_external\n\n"
        image: python:3.10
    exec-merge-adapter:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - merge_adapter
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'transformers'\
          \ 'torch' 'peft' 'accelerate' 'bitsandbytes' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef merge_adapter(base_model_path: str, adapter_path: str, merged_root:\
          \ str) -> str:\n    import torch\n    import os\n    import gc\n    from\
          \ transformers import AutoModelForCausalLM, AutoTokenizer\n    from peft\
          \ import PeftModel\n\n    print(f\"--- Merging Adapter ---\")\n    print(f\"\
          Base: {base_model_path}\")\n    print(f\"Adapter: {adapter_path}\")\n\n\
          \    # 1. Load Base Model in FP16 on CPU\n    # Why CPU? Loading a 4B/7B\
          \ model in FP16 takes 8GB/15GB RAM. \n    # If we do this on an 11GB GPU,\
          \ we might OOM before merging.\n    # System RAM is usually cheaper and\
          \ larger.\n    print(\"Loading base model to System RAM (CPU)...\")\n  \
          \  base_model = AutoModelForCausalLM.from_pretrained(\n        base_model_path,\n\
          \        torch_dtype=torch.float16,\n        device_map=\"cpu\", # Force\
          \ CPU\n        low_cpu_mem_usage=True,\n        trust_remote_code=True\n\
          \    )\n\n    # 2. Load Adapter\n    print(\"Loading LoRA adapter...\")\n\
          \    model = PeftModel.from_pretrained(base_model, adapter_path)\n\n   \
          \ # 3. Merge\n    print(\"Merging weights...\")\n    model = model.merge_and_unload()\n\
          \n    # 4. Save\n    save_path = os.path.join(merged_root, \"qwen_merged_final\"\
          )\n    print(f\"Saving merged model to {save_path}...\")\n    model.save_pretrained(save_path)\n\
          \n    # Save tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(base_model_path,\
          \ trust_remote_code=True)\n    tokenizer.save_pretrained(save_path)\n\n\
          \    # Cleanup\n    del model\n    del base_model\n    gc.collect()\n\n\
          \    return save_path\n\n"
        image: pytorch/pytorch:2.3.0-cuda12.1-cudnn8-runtime
        resources:
          memoryLimit: 25.769803776
          resourceMemoryLimit: 24Gi
    exec-run-inference:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - run_inference
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'transformers'\
          \ 'torch' 'accelerate' 'bitsandbytes' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef run_inference(model_path: str, prompt: str):\n    import torch\n\
          \    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\
          \n    print(f\"Loading merged model from {model_path}...\")\n\n    # Load\
          \ in 4-bit for inference to ensure it fits comfortably in 11GB\n    # even\
          \ with long context windows.\n    bnb_config = BitsAndBytesConfig(\n   \
          \     load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n       \
          \ bnb_4bit_compute_dtype=torch.float16\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(model_path,\
          \ trust_remote_code=True)\n    model = AutoModelForCausalLM.from_pretrained(\n\
          \        model_path,\n        quantization_config=bnb_config,\n        device_map=\"\
          auto\",\n        trust_remote_code=True\n    )\n\n    messages = [\n   \
          \     {\"role\": \"system\", \"content\": \"You are a scholar from the Victorian\
          \ era. Write in an archaic, formal tone.\"},\n        {\"role\": \"user\"\
          , \"content\": prompt}\n    ]\n\n    # Qwen chat template application\n\
          \    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\
          \n    print(f\"Input Prompt: {text}\")\n    inputs = tokenizer(text, return_tensors=\"\
          pt\").to(model.device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n\
          \            **inputs, \n            max_new_tokens=200,\n            do_sample=True,\n\
          \            temperature=0.7,\n            top_p=0.9\n        )\n\n    response\
          \ = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(\"\
          -\" * 30)\n    print(\"MODEL OUTPUT:\")\n    print(\"-\" * 30)\n    print(response)\n\
          \n"
        image: pytorch/pytorch:2.3.0-cuda12.1-cudnn8-runtime
        resources:
          accelerator:
            count: '1'
            resourceCount: '1'
pipelineInfo:
  name: qwen-finetune-production
root:
  dag:
    tasks:
      download-dataset:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-download-dataset
        inputs:
          parameters:
            data_root:
              runtimeValue:
                constant: /mnt/data
            dataset_name:
              componentInputParameter: dataset_name
            force_download:
              componentInputParameter: force_download
            subset_size:
              componentInputParameter: subset_size
        taskInfo:
          name: download-dataset
      download-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-download-model
        inputs:
          parameters:
            force_download:
              componentInputParameter: force_download
            model_name:
              componentInputParameter: model_name
            model_root:
              runtimeValue:
                constant: /mnt/models
        taskInfo:
          name: download-model
      launch-training-job:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-launch-training-job
        dependentTasks:
        - download-dataset
        - download-model
        inputs:
          parameters:
            base_model_path:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: download-model
            data_path:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: download-dataset
            data_pvc:
              componentInputParameter: data_pvc
            image:
              componentInputParameter: training_image_uri
            max_steps:
              componentInputParameter: max_steps
            model_pvc:
              componentInputParameter: model_pvc
            model_root:
              runtimeValue:
                constant: /mnt/models
        taskInfo:
          name: launch-training-job
      merge-adapter:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-merge-adapter
        dependentTasks:
        - download-model
        - launch-training-job
        inputs:
          parameters:
            adapter_path:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: launch-training-job
            base_model_path:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: download-model
            merged_root:
              runtimeValue:
                constant: /mnt/models
        taskInfo:
          name: merge-adapter
      run-inference:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-run-inference
        dependentTasks:
        - merge-adapter
        inputs:
          parameters:
            model_path:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: merge-adapter
            prompt:
              runtimeValue:
                constant: Explain the nature of electricity.
        taskInfo:
          name: run-inference
  inputDefinitions:
    parameters:
      data_pvc:
        defaultValue: llm-data-pvc
        isOptional: true
        parameterType: STRING
      dataset_name:
        defaultValue: deepmind/pg19
        isOptional: true
        parameterType: STRING
      force_download:
        defaultValue: true
        isOptional: true
        parameterType: BOOLEAN
      max_steps:
        defaultValue: 50.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      model_name:
        defaultValue: Qwen/Qwen3-4B-Thinking-2507
        isOptional: true
        parameterType: STRING
      model_pvc:
        defaultValue: llm-workspace-pvc
        isOptional: true
        parameterType: STRING
      subset_size:
        defaultValue: 500.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      training_image_uri:
        defaultValue: kjh123456/qwen-trainer:v16
        isOptional: true
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.13.0
---
platforms:
  kubernetes:
    deploymentSpec:
      executors:
        exec-download-dataset:
          pvcMount:
          - componentInputParameter: data_pvc
            mountPath: /mnt/data
        exec-download-model:
          pvcMount:
          - componentInputParameter: model_pvc
            mountPath: /mnt/models
        exec-merge-adapter:
          pvcMount:
          - componentInputParameter: model_pvc
            mountPath: /mnt/models
        exec-run-inference:
          pvcMount:
          - componentInputParameter: model_pvc
            mountPath: /mnt/models
